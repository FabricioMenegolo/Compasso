{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexecutable\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      6\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mmaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m  \u001b[39m.\u001b[39mappName(\u001b[39m'\u001b[39m\u001b[39mSparkHelloWorld\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m  \u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    " .appName('SparkHelloWorld') \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType, StructField, StringType, IntegerType\n\u001b[1;32m      2\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m  (\u001b[39m\"\u001b[39m\u001b[39mJames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSmith\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m36636\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3000\u001b[39m), \n\u001b[1;32m      4\u001b[0m  (\u001b[39m\"\u001b[39m\u001b[39mMichael\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m40288\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m  (\u001b[39m\"\u001b[39m\u001b[39mJen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBrown\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3000\u001b[39m)\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m     10\u001b[0m  StructField(\u001b[39m\"\u001b[39m\u001b[39mfirstname\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m), \\\n\u001b[1;32m     11\u001b[0m  StructField(\u001b[39m\"\u001b[39m\u001b[39mmiddlename\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m), \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m  StructField(\u001b[39m\"\u001b[39m\u001b[39msalary\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), \u001b[39mTrue\u001b[39;00m) \\\n\u001b[1;32m     16\u001b[0m  ])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "data = [\n",
    " (\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000), \n",
    " (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", -1), \n",
    " (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000), \n",
    " (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000), \n",
    " (\"Jen\", \"Mary\", \"Brown\", None, \"F\", 3000)\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"firstname\", StringType(), True), \\\n",
    " StructField(\"middlename\", StringType(), True), \\\n",
    " StructField(\"lastname\", StringType(), True), \\\n",
    " StructField(\"id\", StringType(), True), \\\n",
    " StructField(\"sex\", StringType(), True), \\\n",
    " StructField(\"salary\", IntegerType(), True) \\\n",
    " ])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col\n\u001b[1;32m      2\u001b[0m df_filtered \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwhere(\n\u001b[1;32m      3\u001b[0m  (col(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misNotNull()) \u001b[39m&\u001b[39m (col(\u001b[39m\"\u001b[39m\u001b[39msalary\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m df_filtered\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_filtered = df.where(\n",
    " (col(\"id\").isNotNull()) & (col(\"salary\") > 0)\n",
    ")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|      fullname|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    JamesSmith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|RobertWilliams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|MariaAnneJones|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "df_full_name = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " concat(col(\"firstname\"), col(\"middlename\"), col(\"lastname\"))\n",
    ")\n",
    "df_full_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    James  Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|Robert  Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_full_name2 = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"middlename\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\"))\n",
    ")\n",
    "df_full_name2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|     James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000| Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.createOrReplaceTempView(\"df\")\n",
    "df_full_name3 = spark.sql(\"\"\"\n",
    " SELECT *,\n",
    " CASE\n",
    " WHEN middlename = '' THEN concat(firstname, \" \", lastname)\n",
    " ELSE concat(firstname, \" \", middlename, \" \", lastname)\n",
    " END AS fullname\n",
    " FROM df\n",
    "\"\"\")\n",
    "df_full_name3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|         fullname|\n",
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|      James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|  Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Jones Jones|\n",
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_full_name4 = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " when(\n",
    " col(\"middlename\") == \"\",\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\")\n",
    " )\n",
    " ).otherwise(\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\")\n",
    " )\n",
    " )\n",
    ")\n",
    "df_full_name4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

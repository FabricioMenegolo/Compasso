{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "\n",
    "#spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkHelloWorld\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Sparknotest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  P| 10|\n",
      "|  M| 20|\n",
      "|  J| 40|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"P\",10),(\"M\",20),(\"J\",40)])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| Id|Nome|\n",
      "+---+----+\n",
      "|  1|   P|\n",
      "|  2|   M|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"Id INT, Nome STRING\"\n",
    "dados = [[1,\"P\"],[2,\"M\"]]\n",
    "df2 = spark.createDataFrame(dados, schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nDataType ink is not supported.(line 1, pos 3)\n\n== SQL ==\nid INK, nome STRING\n---^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m arqschema \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mid INK, nome STRING\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m arqcsv \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39m/home/fmenegolo/Projects/Compasso/Azimute/Sprint_7/Lab05_AWS_Glue/\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, schema\u001b[39m=\u001b[39;49marqschema)\n\u001b[1;32m      4\u001b[0m \u001b[39m#format=\"csv\", sep=\",\", inferSchema=True)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/sql/readwriter.py:496\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcsv\u001b[39m(\n\u001b[1;32m    424\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    425\u001b[0m     path: PathOrPaths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     unescapedQuoteHandling: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    460\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Loads a CSV file and returns the result as a  :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[1;32m    462\u001b[0m \u001b[39m    This function will go through the input once to determine the input schema if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39m    [('_c0', 'string'), ('_c1', 'string')]\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_opts(\n\u001b[1;32m    497\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    498\u001b[0m         sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m    499\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    500\u001b[0m         quote\u001b[39m=\u001b[39;49mquote,\n\u001b[1;32m    501\u001b[0m         escape\u001b[39m=\u001b[39;49mescape,\n\u001b[1;32m    502\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[1;32m    503\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[1;32m    504\u001b[0m         inferSchema\u001b[39m=\u001b[39;49minferSchema,\n\u001b[1;32m    505\u001b[0m         ignoreLeadingWhiteSpace\u001b[39m=\u001b[39;49mignoreLeadingWhiteSpace,\n\u001b[1;32m    506\u001b[0m         ignoreTrailingWhiteSpace\u001b[39m=\u001b[39;49mignoreTrailingWhiteSpace,\n\u001b[1;32m    507\u001b[0m         nullValue\u001b[39m=\u001b[39;49mnullValue,\n\u001b[1;32m    508\u001b[0m         nanValue\u001b[39m=\u001b[39;49mnanValue,\n\u001b[1;32m    509\u001b[0m         positiveInf\u001b[39m=\u001b[39;49mpositiveInf,\n\u001b[1;32m    510\u001b[0m         negativeInf\u001b[39m=\u001b[39;49mnegativeInf,\n\u001b[1;32m    511\u001b[0m         dateFormat\u001b[39m=\u001b[39;49mdateFormat,\n\u001b[1;32m    512\u001b[0m         timestampFormat\u001b[39m=\u001b[39;49mtimestampFormat,\n\u001b[1;32m    513\u001b[0m         maxColumns\u001b[39m=\u001b[39;49mmaxColumns,\n\u001b[1;32m    514\u001b[0m         maxCharsPerColumn\u001b[39m=\u001b[39;49mmaxCharsPerColumn,\n\u001b[1;32m    515\u001b[0m         maxMalformedLogPerPartition\u001b[39m=\u001b[39;49mmaxMalformedLogPerPartition,\n\u001b[1;32m    516\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    517\u001b[0m         columnNameOfCorruptRecord\u001b[39m=\u001b[39;49mcolumnNameOfCorruptRecord,\n\u001b[1;32m    518\u001b[0m         multiLine\u001b[39m=\u001b[39;49mmultiLine,\n\u001b[1;32m    519\u001b[0m         charToEscapeQuoteEscaping\u001b[39m=\u001b[39;49mcharToEscapeQuoteEscaping,\n\u001b[1;32m    520\u001b[0m         samplingRatio\u001b[39m=\u001b[39;49msamplingRatio,\n\u001b[1;32m    521\u001b[0m         enforceSchema\u001b[39m=\u001b[39;49menforceSchema,\n\u001b[1;32m    522\u001b[0m         emptyValue\u001b[39m=\u001b[39;49memptyValue,\n\u001b[1;32m    523\u001b[0m         locale\u001b[39m=\u001b[39;49mlocale,\n\u001b[1;32m    524\u001b[0m         lineSep\u001b[39m=\u001b[39;49mlineSep,\n\u001b[1;32m    525\u001b[0m         pathGlobFilter\u001b[39m=\u001b[39;49mpathGlobFilter,\n\u001b[1;32m    526\u001b[0m         recursiveFileLookup\u001b[39m=\u001b[39;49mrecursiveFileLookup,\n\u001b[1;32m    527\u001b[0m         modifiedBefore\u001b[39m=\u001b[39;49mmodifiedBefore,\n\u001b[1;32m    528\u001b[0m         modifiedAfter\u001b[39m=\u001b[39;49mmodifiedAfter,\n\u001b[1;32m    529\u001b[0m         unescapedQuoteHandling\u001b[39m=\u001b[39;49munescapedQuoteHandling,\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    532\u001b[0m         path \u001b[39m=\u001b[39m [path]\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/sql/readwriter.py:50\u001b[0m, in \u001b[0;36mOptionUtils._set_opts\u001b[0;34m(self, schema, **options)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mSet named options (filter out those the value is None)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema(schema)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m options\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/sql/readwriter.py:118\u001b[0m, in \u001b[0;36mDataFrameReader.schema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mschema(jschema)\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mschema(schema)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mschema should be StructType or string\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nDataType ink is not supported.(line 1, pos 3)\n\n== SQL ==\nid INK, nome STRING\n---^^^\n"
     ]
    }
   ],
   "source": [
    "arqschema = \"id INK, nome STRING\"\n",
    "arqcsv = spark.read.csv(\"/home/fmenegolo/Projects/Compasso/Azimute/Sprint_7/Lab05_AWS_Glue/\", header=False, schema=arqschema)\n",
    "\n",
    "#format=\"csv\", sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arqcsv.write.format(\"csv\").save(\"/home/fmenegolo/Projects/Compasso/Azimute/Sprint_7/Lab05_AWS_Glue/nomepastacsv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = spark.read.format(\"csv\").load(\"/home/fmenegolo/Projects/Compasso/Azimute/Sprint_7/Lab05_AWS_Glue/nomepastacsv/nomearq.csv\")\n",
    "\n",
    "cs = spark.read.format(\"csv\").load(\"/home/fmenegolo/Projects/Compasso/Azimute/Sprint_7/Lab05_AWS_Glue/nomepastacsv/nomearq.csv\", schema=arqschema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
